import collections
import typing
import warnings

import PIL.Image
import ipyplot
import numpy as np
import pandas as pd
import seaborn as sns
import torch
from matplotlib import pyplot as plt

_ArrayLike = np.ndarray | torch.Tensor | typing.Iterable
_DistributionT = typing.TypeVar("_DistributionT")


def plot_histogram_from_points(
        data: list[tuple[int, int]],
        title: str = "Histogram of the distribution generated by a drawing",
        xlabel: str = "Y-Axis",
        ylabel: str = "X-Axis",
        histplot_kwargs: typing.Optional[dict] = None,
):
    # Instance the kwargs of the histplot and set default values.
    histplot_kwargs = dict() if histplot_kwargs is None else histplot_kwargs
    histplot_kwargs.setdefault("bins", 100)
    histplot_kwargs.setdefault("cbar", True)

    df = pd.DataFrame(data)
    histplot_return = sns.histplot(data=df, x=0, y=1, **histplot_kwargs)
    plt.xlabel(xlabel)
    plt.ylabel(ylabel)
    plt.title(title)

    return histplot_return


def plot_list_of_images(list_of_images: list[PIL.Image.Image], **kwargs):
    """
    Function that plots a list of images.

    :param list_of_images: The list of images to draw.
    :param kwargs: Optional arguments to pass to the ipyplot.plot_images function. For further
        information, please see the documentation of that function.
    """
    # Set values by default
    kwargs.setdefault("max_images", 36)
    kwargs.setdefault("img_width", 75)

    with warnings.catch_warnings():
        warnings.simplefilter("ignore")
        ipyplot.plot_images(list_of_images, **kwargs)


def plot_list_of_draws(list_of_draws, **kwargs):
    """
    Function that plots a list of DistributionDraws instances.

    :param list_of_draws: The list of distributions to draw.
    :param kwargs: Optional arguments to pass to the ipyplot.plot_images function. For further
        information, please see the documentation of that function.
    """
    return plot_list_of_images([draw.image for draw in list_of_draws], **kwargs)


def freq_labels_posterior(posterior) -> typing.Sequence[str]:
    return [f"id: {id_},\nfreq: {freq}" for id_, freq in posterior.samples_counter.most_common()]


def likelihood_ordered_dict(mcmc):
    like_cache = mcmc.likelihood_cache
    posterior_probs = like_cache / np.sum(like_cache)
    likelihood_dct = collections.OrderedDict({i: prob for i, prob in enumerate(posterior_probs)})

    for key, _ in sorted(likelihood_dct.items(), key=lambda item: -item[1]):
        likelihood_dct.move_to_end(key)

    return likelihood_dct


def normalised_steps_ordered_dict(mcmc):
    counter = mcmc.steps_counter
    return collections.OrderedDict([(k, v / counter.total()) for k, v in counter.most_common()])


def normalised_samples_ordered_dict(mcmc):
    counter = mcmc.samples_counter
    return collections.OrderedDict([(k, v / counter.total()) for k, v in counter.most_common()])


@torch.jit.script
def _grayscale(
        to_return: torch.Tensor,
        weights: torch.Tensor,
        support: torch.Tensor,
) -> torch.Tensor:
    support = torch.round(support).type(torch.int32)
    support1, support2 = support[:, 0], support[:, 1]
    for w, pos1, pos2 in zip(weights, support1, support2):
        to_return[pos1, pos2] += w
    to_return = (to_return / torch.max(to_return) * 255).type(torch.uint8)
    return to_return


@torch.jit.script
def _partition(
        X: torch.Tensor,
        mu: torch.Tensor,
        alpha,
) -> tuple[torch.Tensor, torch.Tensor]:
    _, n_dim = X.shape
    min_w = torch.min(mu)

    n_times = torch.ceil(alpha * mu / min_w).type(torch.int).to(mu.device)
    n_rows = int(torch.sum(n_times))

    X_ = torch.zeros((n_rows, n_dim), dtype=X.dtype, device=X.device)
    mu_ = torch.zeros((n_rows,), dtype=mu.dtype, device=mu.device)
    i = 0
    for x, w, n in zip(X, mu, n_times):
        x, w = x, w / n
        for _ in range(int(n)):
            X_[i] = x
            mu_[i] = w
            i += 1

    return X_, mu_


def partition(X: torch.Tensor, mu: torch.Tensor, alpha: float):
    X, mu = _partition(X, mu, torch.tensor(alpha))
    mu = mu / torch.sum(mu)

    return X, mu
