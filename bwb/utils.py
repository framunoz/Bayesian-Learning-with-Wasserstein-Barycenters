import collections
import typing
import warnings

import PIL.Image
import ipyplot
import numba
import numpy as np
import pandas as pd
import seaborn as sns
import torch
from matplotlib import pyplot as plt

_ArrayLike = np.ndarray | torch.Tensor | typing.Iterable
_DistributionT = typing.TypeVar("_DistributionT")


def plot_histogram_from_points(
        data: list[tuple[int, int]],
        title: str = "Histogram of the distribution generated by a drawing",
        xlabel: str = "Y-Axis",
        ylabel: str = "X-Axis",
        histplot_kwargs: typing.Optional[dict] = None,
):
    # Instance the kwargs of the histplot and set default values.
    histplot_kwargs = dict() if histplot_kwargs is None else histplot_kwargs
    histplot_kwargs.setdefault("bins", 100)
    histplot_kwargs.setdefault("cbar", True)

    df = pd.DataFrame(data)
    histplot_return = sns.histplot(data=df, x=0, y=1, **histplot_kwargs)
    plt.xlabel(xlabel)
    plt.ylabel(ylabel)
    plt.title(title)

    return histplot_return


def plot_list_of_draws(list_of_draws, **kwargs):
    """
    Function that plots a list of DistributionDraws instances.

    :param list_of_draws: The list of distributions to draw.
    :param kwargs: Optional arguments to pass to the ipyplot.plot_images function. For further
        information, please see the documentation of that function.
    """
    # Map the list of draws to obtain a list of images
    list_of_images: list[PIL.Image.Image] = [draw.image for draw in list_of_draws]

    # Set values by default
    kwargs.setdefault("max_images", 36)
    kwargs.setdefault("img_width", 75)

    with warnings.catch_warnings():
        warnings.simplefilter("ignore")
        ipyplot.plot_images(list_of_images, **kwargs)


def freq_labels_posterior(posterior) -> typing.Sequence[str]:
    return [f"id: {id_},\nfreq: {freq}" for id_, freq in posterior.samples_counter.most_common()]


def likelihood_ordered_dict(mcmc):
    like_cache = mcmc.likelihood_cache
    posterior_probs = like_cache / np.sum(like_cache)
    likelihood_dct = collections.OrderedDict({i: prob for i, prob in enumerate(posterior_probs)})

    for key, _ in sorted(likelihood_dct.items(), key=lambda item: -item[1]):
        likelihood_dct.move_to_end(key)

    return likelihood_dct


def normalised_steps_ordered_dict(mcmc):
    counter = mcmc.steps_counter
    return collections.OrderedDict([(k, v / counter.total()) for k, v in counter.most_common()])


def normalised_samples_ordered_dict(mcmc):
    counter = mcmc.samples_counter
    return collections.OrderedDict([(k, v / counter.total()) for k, v in counter.most_common()])


@numba.jit(nopython=True)
def _grayscale(shape: tuple, weights: np.ndarray, support: np.ndarray) -> np.ndarray:
    to_return: np.ndarray = np.zeros(shape)
    support = np.round(support).astype("int32")
    support1, support2 = support[:, 0], support[:, 1]
    for w, pos1, pos2 in zip(weights, support1, support2):
        to_return[pos1, pos2] += w
    to_return: np.ndarray = (to_return / np.max(to_return) * 255).astype("uint8")
    return to_return
