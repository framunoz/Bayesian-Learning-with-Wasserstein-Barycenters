import collections as c
import typing as t
import warnings

import ipyplot
import numpy as np
import pandas as pd
import PIL.Image
import seaborn as sns
import torch
from matplotlib import pyplot as plt

import bwb.logging as logging
from bwb.config import config

_ArrayLike = np.ndarray | torch.Tensor | t.Iterable
_DistributionT = t.TypeVar("_DistributionT")
_log = logging.get_logger(__name__)


def plot_histogram_from_points(
    data: list[tuple[int, int]],
    title: str = "Histogram of the distribution generated by a drawing",
    xlabel: str = "Y-Axis",
    ylabel: str = "X-Axis",
    histplot_kwargs: t.Optional[dict] = None,
):
    # Instance the kwargs of the histplot and set default values.
    histplot_kwargs = dict() if histplot_kwargs is None else histplot_kwargs
    histplot_kwargs.setdefault("bins", 100)
    histplot_kwargs.setdefault("cbar", True)

    df = pd.DataFrame(data)
    histplot_return = sns.histplot(data=df, x=0, y=1, **histplot_kwargs)
    plt.xlabel(xlabel)
    plt.ylabel(ylabel)
    plt.title(title)

    return histplot_return


def plot_list_of_images(list_of_images: list[PIL.Image.Image], **kwargs):
    """
    Function that plots a list of images.

    :param list_of_images: The list of images to draw.
    :param kwargs: Optional arguments to pass to the ipyplot.plot_images function. For further
        information, please see the documentation of that function.
    """
    # Set values by default
    kwargs.setdefault("max_images", 36)
    kwargs.setdefault("img_width", 75)

    with warnings.catch_warnings():
        warnings.simplefilter("ignore")
        ipyplot.plot_images(list_of_images, **kwargs)


def plot_list_of_draws(list_of_draws, **kwargs):
    """
    Function that plots a list of DistributionDraws instances.

    :param list_of_draws: The list of distributions to draw.
    :param kwargs: Optional arguments to pass to the ipyplot.plot_images function. For further
        information, please see the documentation of that function.
    """
    return plot_list_of_images([draw.image for draw in list_of_draws], **kwargs)


def freq_labels_posterior(posterior) -> t.Sequence[str]:
    return [
        f"id: {id_},\nfreq: {freq}"
        for id_, freq in posterior.samples_counter.most_common()
    ]


def likelihood_ordered_dict(mcmc):
    like_cache = mcmc.likelihood_cache
    posterior_probs = like_cache / np.sum(like_cache)
    likelihood_dct = c.OrderedDict({i: prob for i, prob in enumerate(posterior_probs)})

    for key, _ in sorted(likelihood_dct.items(), key=lambda item: -item[1]):
        likelihood_dct.move_to_end(key)

    return likelihood_dct


def normalised_steps_ordered_dict(mcmc):
    counter = mcmc.steps_counter
    return c.OrderedDict([(k, v / counter.total()) for k, v in counter.most_common()])


def normalised_samples_ordered_dict(mcmc):
    counter = mcmc.samples_counter
    return c.OrderedDict([(k, v / counter.total()) for k, v in counter.most_common()])


def __grayscale(
    to_return: torch.Tensor,
    weights: torch.Tensor,
    support: torch.Tensor,
) -> torch.Tensor:
    support = torch.round(support)  # .type(torch.int32)
    support1, support2 = support[:, 0], support[:, 1]
    for w, pos1, pos2 in zip(weights, support1, support2):
        to_return[pos1, pos2] += w
    to_return = (to_return / torch.max(to_return) * 255)  # .type(torch.uint8)
    return to_return


_grayscale = torch.jit.script(
    __grayscale,
    example_inputs=[(
        torch.rand((28, 28), dtype=torch.float32, device=config.device),  # to_return
        torch.rand((784,), dtype=torch.float32, device=config.device),  # weights
        torch.randint(-10, 10, size=(784, 2), dtype=torch.int32, device=config.device),  # support
    ), (
        torch.rand((28, 28), dtype=torch.float64, device=config.device),  # to_return
        torch.rand((784,), dtype=torch.float64, device=config.device),  # weights
        torch.randint(-10, 10, size=(784, 2), dtype=torch.int32, device=config.device),  # support
    )]
)


@torch.jit.script
def _partition(
    X: torch.Tensor,
    mu: torch.Tensor,
    alpha,
) -> tuple[torch.Tensor, torch.Tensor]:
    _, n_dim = X.shape
    min_w = torch.min(mu)

    n_times = torch.ceil(alpha * mu / min_w).type(torch.int).to(mu.device)
    n_rows = int(torch.sum(n_times))

    X_ = torch.zeros((n_rows, n_dim), dtype=X.dtype, device=X.device)
    mu_ = torch.zeros((n_rows,), dtype=mu.dtype, device=mu.device)
    i = 0
    for x, w, n in zip(X, mu, n_times):
        x, w = x, w / n
        for _ in range(int(n)):
            X_[i] = x
            mu_[i] = w
            i += 1

    return X_, mu_


def partition(X: torch.Tensor, mu: torch.Tensor, alpha: float):
    alpha = torch.tensor(alpha)

    if alpha <= 0:
        raise ValueError("The alpha parameter must be greater than 0")

    if _log.level <= logging.INFO:
        n_times = torch.ceil(alpha * mu / torch.min(mu)).type(torch.int).to(mu.device)
        _log.debug(f"Number of times to repeat each sample: {n_times}")
        n_rows = int(torch.sum(n_times))
        _log.info(f"Number of rows in the new X: {n_rows}")

    X, mu = _partition(X, mu, alpha)
    mu = mu / torch.sum(mu)

    return X, mu
