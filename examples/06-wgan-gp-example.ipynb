{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "source": [
    "%cd ~/codeProjects/pythonProjects/Bayesian-Learning-with-Wasserstein-Barycenters\n",
    "# %cd D:\\CodeProjects\\Python\\Bayesian-Learning-with-Wasserstein-Barycenters\\\n",
    "# Soy una nueva linea"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "source": [
    "from wgan_gp.wgan_gp_vae.model_resnet import Generator, Encoder"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "source": [
    "LATENT_DIM = 128\n",
    "CHANNELS_IMG = 1\n",
    "NUM_FILTERS = [256, 128, 64, 32]\n",
    "\n",
    "G = Generator(LATENT_DIM, CHANNELS_IMG).to(device)\n",
    "E = Encoder(LATENT_DIM, CHANNELS_IMG).to(device)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "CURR_PATH = Path(\".\")\n",
    "NETS_PATH = CURR_PATH / \"wgan_gp\" / \"networks\" \n",
    "# FACE_PATH = NETS_PATH / \"face_zDim128_gauss_recognized_augmented_WAE_WGAN_loss_mse_64p64\"\n",
    "FACE_PATH = NETS_PATH / \"_resnet_face_zDim128_gauss_bs_128_recognized_augmented_WAE_WGAN_loss_l1_32p32\"\n",
    "# FACE_PATH = NETS_PATH / \"face_zDim128_gauss_recognized_augmented_WAE_WGAN_64p64\"\n",
    "# FACE_PATH = NETS_PATH / \"face_zDim128_gauss_recognized_augmented_64p64\"\n",
    "Path(\"./wgan_gp/networks/\")\n",
    "\n",
    "DATA_PATH = CURR_PATH / \"data\" / \"face_recognized.npy\"\n",
    "\n",
    "\n",
    "DATA_PATH, FACE_PATH"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "source": [
    "from wgan_gp.wgan_gp_vae.utils import load_checkpoint\n",
    "\n",
    "load_checkpoint(G, FACE_PATH, \"generator\", device)\n",
    "load_checkpoint(E, FACE_PATH, \"encoder\", device)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "source": [
    "\n",
    "from bwb.distributions.data_loaders import *"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "\n",
    "arr = np.load(DATA_PATH)\n",
    "arr.shape"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "source": [
    "faces = DistributionDrawDataLoader(arr, (28, 28))\n",
    "faces"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "source": [
    "first_face = faces[0]\n",
    "first_face"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform_in = transforms.Compose(\n",
    "    [\n",
    "        # From pdf to grayscale\n",
    "        transforms.Lambda(lambda x: x / torch.max(x)),\n",
    "        # transforms.Lambda(lambda x: x),\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((32, 32)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            [0.5 for _ in range(1)],\n",
    "            [0.5 for _ in range(1)],\n",
    "            ),\n",
    "    ]\n",
    ")\n",
    "transform_in(first_face.grayscale_weights)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "source": [
    "from wgan_gp.wgan_gp_vae.utils import ProjectorOnManifold\n",
    "\n",
    "proj = ProjectorOnManifold(\n",
    "    E, G, \n",
    "    transform_in=transforms.Compose(\n",
    "            [\n",
    "                # From pdf to grayscale\n",
    "                transforms.Lambda(lambda x: x / torch.max(x)),\n",
    "                # transforms.Lambda(lambda x: x),\n",
    "                transforms.ToPILImage(),\n",
    "                transforms.Resize((32, 32)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    [0.5 for _ in range(1)],\n",
    "                    [0.5 for _ in range(1)],\n",
    "                ),\n",
    "            ]\n",
    "        ),\n",
    "    transform_out=transforms.Compose(\n",
    "            [\n",
    "                # Ensure the range is in [0, 1]\n",
    "                transforms.Lambda(lambda x: x - torch.min(x)),\n",
    "                transforms.Lambda(lambda x: x / torch.max(x)),\n",
    "                transforms.Lambda(lambda x: 1 - x),\n",
    "                transforms.ToPILImage(),\n",
    "                # transforms.Resize((28, 28)),\n",
    "                # transforms.ToTensor(),\n",
    "                # transforms.Lambda(lambda x: x / torch.sum(x)),\n",
    "                # transforms.Lambda(lambda x: x.squeeze(0)),\n",
    "            ]\n",
    "        ))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "source": [
    "proj._transform_in(first_face.grayscale_weights)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "source": [
    "import requests\n",
    "import io\n",
    "\n",
    "response = requests.get('https://storage.googleapis.com/quickdraw_dataset/full/numpy_bitmap/cat.npy')\n",
    "response.raise_for_status()\n",
    "data = np.load(io.BytesIO(response.content))  # Works!"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "source": [
    "cats = DistributionDrawDataLoader(data, (28, 28))\n",
    "cats[1]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "source": [
    "cat = cats[9]\n",
    "cat"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "source": [
    "proj(cat.grayscale_weights)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "source": [
    "from quickdraw_dataset import QuickDraw\n",
    "import torchvision.transforms as T\n",
    "from bwb.distributions import DistributionDraw\n",
    "\n",
    "ds = QuickDraw(\n",
    "    Path(\"./data\"), \n",
    "    category=\"cat\", \n",
    "    recognized=True, \n",
    "    download=True, \n",
    "    transform=T.Compose([\n",
    "        T.ToTensor(),\n",
    "        T.Lambda(lambda x: x.squeeze()),\n",
    "        T.Lambda(lambda x: DistributionDraw.from_grayscale_weights(x))\n",
    "    ])\n",
    ")\n",
    "ds[0][0]\n",
    "# proj(ds[0][0])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "source": [
    "max_images = 4 * 9\n",
    "cats_list = []\n",
    "cats_projected_list = []\n",
    "for k in range(max_images):\n",
    "    cat = ds[k][0]\n",
    "    cats_list.append(cat)\n",
    "    cats_projected_list.append(proj(cat.grayscale_weights))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "source": [
    "from bwb import utils\n",
    "\n",
    "\n",
    "utils.plot_list_of_draws(cats_list, max_images=max_images)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "source": [
    "utils.plot_list_of_images(cats_projected_list, max_images=max_images)"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
