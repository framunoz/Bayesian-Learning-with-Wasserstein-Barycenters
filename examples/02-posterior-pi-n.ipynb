{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ~/codeProjects/pythonProjects/Bayesian-Learning-with-Wasserstein-Barycenters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bwb import _logging as logging\n",
    "\n",
    "\n",
    "RUN_MCMC = True\n",
    "\n",
    "log = logging.get_logger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Posterior ExplÃ­cita"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from bwb.distributions.data_loaders import DistributionDrawDataLoader\n",
    "from bwb.distributions import ExplicitPosteriorSampler\n",
    "\n",
    "ExplicitPosteriorSampler.set_save_samples(True)\n",
    "\n",
    "expl_posterior = ExplicitPosteriorSampler()\n",
    "expl_posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "data_path = Path(\"./data/face.npy\")\n",
    "\n",
    "models_array = np.load(data_path)\n",
    "n_faces, _ = models_array.shape\n",
    "print(f\"{n_faces = }\")\n",
    "\n",
    "faces = DistributionDrawDataLoader(models_array, (28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "face0 = faces[0]\n",
    "face0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "data = faces[0].sample((100,))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face0.enumerate_support_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bwb.plotters as plotters\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_coords = (\n",
    "    face0.enumerate_support_()[data].cpu().numpy()\n",
    "    + np.random.randn(100, 2) * 0.1\n",
    ")\n",
    "# data_coords = face0.enumerate_support_()[face0.sample((10_000,))].cpu().numpy() + np.random.randn(10_000, 2) * 0.1\n",
    "\n",
    "plotters.plot_histogram_from_points(\n",
    "    data_coords, rotate=True, histplot_kwargs=dict(bins=28)\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_5 = data[:5]\n",
    "data_50 = data[:50]\n",
    "data_100 = data[:100]\n",
    "data_coords_5 = data_coords[:5]\n",
    "data_coords_50 = data_coords[:50]\n",
    "data_coords_100 = data_coords[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_images = 3 * 13\n",
    "expl_posterior = ExplicitPosteriorSampler()\n",
    "expl_posterior.fit(data=data_5, models=faces)\n",
    "expl_posterior.rvs(1_000, seed=42)\n",
    "\n",
    "print(f\"Total time: {expl_posterior.total_time:.2f} seconds\")\n",
    "\n",
    "expl_posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bwb.utils as utils\n",
    "\n",
    "most_common = [\n",
    "    faces[i] for i, _ in expl_posterior.samples_counter.most_common(max_images)\n",
    "]\n",
    "plotters.plot_list_of_draws(\n",
    "    most_common,\n",
    "    labels=utils.freq_labels_dist_sampler(expl_posterior),\n",
    "    cmap=\"binary_r\",\n",
    ")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_images = 3 * 13\n",
    "expl_posterior = ExplicitPosteriorSampler()\n",
    "expl_posterior.fit(data=data[:10], models=faces)\n",
    "expl_posterior.rvs(1_000, seed=42)\n",
    "print(f\"Total time: {expl_posterior.total_time:.2f} seconds\")\n",
    "most_common = [\n",
    "    faces[i] for i, _ in expl_posterior.samples_counter.most_common(max_images)\n",
    "]\n",
    "plotters.plot_list_of_draws(\n",
    "    most_common,\n",
    "    labels=utils.freq_labels_dist_sampler(expl_posterior),\n",
    ")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_images = 3 * 13\n",
    "expl_posterior = ExplicitPosteriorSampler()\n",
    "expl_posterior.fit(data=data[:20], models=faces)\n",
    "expl_posterior.rvs(1_000, seed=42)\n",
    "print(f\"Total time: {expl_posterior.total_time:.2f} seconds\")\n",
    "most_common = [\n",
    "    faces[i] for i, _ in expl_posterior.samples_counter.most_common(max_images)\n",
    "]\n",
    "plotters.plot_list_of_draws(\n",
    "    most_common,\n",
    "    labels=utils.freq_labels_dist_sampler(expl_posterior),\n",
    ")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bwb.distributions.distribution_samplers import UniformDiscreteSampler\n",
    "\n",
    "max_images = 3 * 13\n",
    "uniform_sampler = UniformDiscreteSampler()\n",
    "uniform_sampler.fit(models=faces)\n",
    "uniform_sampler.rvs(1_000, seed=42)\n",
    "print(f\"Total time: {uniform_sampler.total_time:.2f} seconds\")\n",
    "most_common = [\n",
    "    faces[i] for i, _ in uniform_sampler.samples_counter.most_common(max_images)\n",
    "]\n",
    "plotters.plot_list_of_draws(\n",
    "    most_common,\n",
    "    labels=utils.freq_labels_dist_sampler(uniform_sampler),\n",
    ")\n",
    "uniform_sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_dict = dict(a=1, b=2)\n",
    "old_dict.setdefault(\"c\", 2)\n",
    "new_dict = dict(a=2, c=4)\n",
    "old_dict.update(new_dict)\n",
    "old_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from quick_torch import QuickDraw\n",
    "import torchvision.transforms.v2 as T\n",
    "from pathlib import Path\n",
    "\n",
    "# noinspection PyProtectedMember\n",
    "from bwb.utils import array_like_t\n",
    "from torch.utils.data import DataLoader\n",
    "from bwb.config import config\n",
    "import multiprocessing as mp\n",
    "import bwb.distributions as dist\n",
    "from bwb.distributions.models import BaseDiscreteWeightedModelSet\n",
    "\n",
    "ds = QuickDraw(\n",
    "    Path(\"./data\"),\n",
    "    categories=\"face\",\n",
    "    download=True,\n",
    "    transform=T.Compose([\n",
    "        T.ToTensor(),\n",
    "        T.Lambda(lambda x: x.squeeze()),\n",
    "    ]),\n",
    ")\n",
    "\n",
    "\n",
    "# noinspection PyMethodOverriding,PyShadowingNames\n",
    "class DatasetWrapper(BaseDiscreteWeightedModelSet):\n",
    "    def __init__(\n",
    "        self, dataset, dataloader_args=None, device=None, dtype=None, eps=None\n",
    "    ):\n",
    "        self.dataset = dataset\n",
    "\n",
    "        self.dataloader_args = dataloader_args or dict()\n",
    "        default_args = dict(\n",
    "            batch_size=1024, shuffle=False, num_workers=mp.cpu_count()\n",
    "        )\n",
    "        for key, value in default_args.items():\n",
    "            self.dataloader_args.setdefault(key, value)\n",
    "\n",
    "        self.device = torch.device(device or config.device)\n",
    "        # If we are working in cuda\n",
    "        if self.device.type == \"cuda\":\n",
    "            self.dataloader_args.setdefault(\"pin_memory\", True)\n",
    "\n",
    "        self.dtype = dtype or config.dtype\n",
    "\n",
    "        self.eps = eps or config.eps\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def compute_likelihood(self, data: array_like_t, **kwargs) -> torch.Tensor:\n",
    "        dataloader = DataLoader(self.dataset, **self.dataloader_args)\n",
    "        data = torch.as_tensor(data, device=self.device).reshape(1, -1)\n",
    "\n",
    "        likelihoods = []\n",
    "\n",
    "        for features, _ in dataloader:\n",
    "            # Transfer the features to the appropriate device\n",
    "            features = torch.as_tensor(\n",
    "                features, device=self.device, dtype=self.dtype\n",
    "            )\n",
    "            # Flatten the features\n",
    "            features = features.reshape(features.size(0), -1)\n",
    "            # Normalize the features\n",
    "            features = features / features.sum(dim=1, keepdim=True)\n",
    "            # Compute the log of the features\n",
    "            features = torch.log(features + self.eps)\n",
    "            # Compute the likelihood\n",
    "            evaluations = torch.take_along_dim(features, data, 1)\n",
    "            likelihood = torch.exp(evaluations.sum(dim=1))\n",
    "\n",
    "            likelihoods.append(likelihood)\n",
    "\n",
    "        likelihood_cache = torch.cat(likelihoods, dim=0)\n",
    "\n",
    "        probabilities = likelihood_cache / (likelihood_cache.sum() + self.eps)\n",
    "\n",
    "        return probabilities\n",
    "\n",
    "    def get(self, i: int, **kwargs) -> dist.DistributionDraw:\n",
    "        return dist.DistributionDraw.from_grayscale_weights(self.dataset[i][0])\n",
    "\n",
    "\n",
    "ds_wrapped = DatasetWrapper(ds)\n",
    "\n",
    "ds_wrapped.get(1)\n",
    "ds_wrapped.compute_likelihood(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_images = 3 * 13\n",
    "expl_posterior = ExplicitPosteriorSampler(save_samples=True)\n",
    "expl_posterior.fit(data=data[:20], models=ds_wrapped)\n",
    "print(f\"Total time: {expl_posterior.total_time:.2f} seconds\")\n",
    "expl_posterior.rvs(1_000, seed=42)\n",
    "print(f\"Total time: {expl_posterior.total_time:.2f} seconds\")\n",
    "most_common = [\n",
    "    faces[i] for i, _ in expl_posterior.samples_counter.most_common(max_images)\n",
    "]\n",
    "plotters.plot_list_of_draws(\n",
    "    most_common,\n",
    "    labels=utils.freq_labels_dist_sampler(expl_posterior),\n",
    ")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expl_posterior.models_.get(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "expl_posterior.fit(data=data, models=faces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "expl_posterior.rvs(size=1000, seed=42)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "expl_posterior.total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "expl_posterior2 = ExplicitPosteriorSampler()\n",
    "expl_posterior2.total_time, expl_posterior.total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    expl_posterior2.draw()\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "len(expl_posterior.samples_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(expl_posterior.samples_counter.total())\n",
    "expl_posterior.draw()\n",
    "print(expl_posterior.samples_counter.total())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "expl_posterior.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del expl_posterior, expl_posterior2, faces, models_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importar redes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wgan_gp.wgan_gp_vae.model_resnet import (\n",
    "    Generator,\n",
    "    Encoder,\n",
    "    LatentDistribution,\n",
    ")\n",
    "import torch\n",
    "from wgan_gp.wgan_gp_vae.utils import load_checkpoint\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "NOISE = \"norm\"\n",
    "LATENT_DIM = 128\n",
    "CHANNELS_IMG = 1\n",
    "NUM_FILTERS = [256, 128, 64, 32]\n",
    "\n",
    "noise_sampler = LatentDistribution(NOISE, LATENT_DIM, device)\n",
    "\n",
    "CURR_PATH = Path(\".\")\n",
    "NETS_PATH = CURR_PATH / \"wgan_gp\" / \"networks\"\n",
    "\n",
    "DS_NAME = \"data\"\n",
    "\n",
    "G = Generator(LATENT_DIM, CHANNELS_IMG, latent_distr=NOISE).to(device)\n",
    "E = Encoder(LATENT_DIM, CHANNELS_IMG).to(device)\n",
    "\n",
    "FACE_PATH = NETS_PATH / f\"cleaned_{DS_NAME}_zDim{LATENT_DIM}_{NOISE}_bs_128\"\n",
    "\n",
    "load_checkpoint(G, FACE_PATH, \"generator\", device)\n",
    "load_checkpoint(E, FACE_PATH, \"encoder\", device)\n",
    "\n",
    "G.eval()\n",
    "E.eval()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bwb.distributions import DistributionDraw\n",
    "from torchvision import disable_beta_transforms_warning\n",
    "\n",
    "disable_beta_transforms_warning()\n",
    "\n",
    "import torchvision.transforms.v2 as T\n",
    "\n",
    "\n",
    "z = noise_sampler(1)\n",
    "m = G(z)\n",
    "\n",
    "transform_in = T.Compose([\n",
    "    T.Lambda(lambda x: x / torch.max(x)),\n",
    "    T.ToPILImage(),\n",
    "    T.Resize(32),\n",
    "    T.ToImage(),\n",
    "    T.ConvertImageDtype(torch.float32),\n",
    "    T.Normalize((0.5,), (0.5,)),\n",
    "])\n",
    "\n",
    "transform_out_ = T.Compose([\n",
    "    T.ToDtype(torch.float64),\n",
    "    T.Lambda(lambda x: x.squeeze()),\n",
    "    T.Lambda(lambda x: x - torch.min(x)),\n",
    "    T.Lambda(lambda x: x / torch.sum(x)),\n",
    "    # T.Lambda(lambda x: DistributionDraw.from_grayscale_weights(x)),\n",
    "])\n",
    "\n",
    "transform_out = T.Compose([\n",
    "    transform_out_,\n",
    "    T.Lambda(lambda x: DistributionDraw.from_grayscale_weights(x)),\n",
    "])\n",
    "\n",
    "out: DistributionDraw = transform_out(m)\n",
    "print(out.dtype)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bwb.distributions.distribution_samplers import GeneratorDistribSampler\n",
    "\n",
    "distr_sampler = GeneratorDistribSampler()\n",
    "distr_sampler.fit(G, transform_out_, noise_sampler)\n",
    "distr_sampler.transform_noise(z)\n",
    "distr_sampler.rvs(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distr_sampler.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 4102165607616432379\n",
    "print(f\"{seed = }\")\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "with torch.no_grad():\n",
    "    m = transform_in(face0.grayscale_weights).unsqueeze(0).to(device)\n",
    "    z = E(m)\n",
    "    m = G(z)\n",
    "\n",
    "face = transform_out(m)\n",
    "\n",
    "# face = bwb_dist.DistributionDraw.from_grayscale_weights(m)\n",
    "data = face.sample((100,))\n",
    "print(f\"{data = }\")\n",
    "face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = face.shape\n",
    "data_coords = (\n",
    "    face.enumerate_support_()[data].cpu().numpy()\n",
    "    + np.random.randn(len(data), 2) * 0.1\n",
    ")\n",
    "\n",
    "plotters.plot_histogram_from_points(\n",
    "    data_coords, rotate=True, shape=shape, histplot_kwargs=dict(bins=28)\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCMC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://quicklatex.com/cache3/e9/ql_ee61247290f642a5afc1fc6205cc98e9_l3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizaremos MCMC utilizando la librerÃ­a pyro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyro.infer.mcmc import NUTS, MCMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEV = \"cpu\"\n",
    "# means1 = torch.tensor([-5, 5], device=DEV)\n",
    "# means2 = -means1\n",
    "#\n",
    "# ndim = means1.size(0)\n",
    "# cov = torch.eye(ndim, device=DEV) * 5\n",
    "#\n",
    "# # @torch.jit.script\n",
    "# def prob(\n",
    "#     params: dict[str, torch.Tensor],\n",
    "#     means1: torch.Tensor = means1,\n",
    "#     means2: torch.Tensor = means2,\n",
    "#     cov: torch.Tensor = cov,\n",
    "#     const: torch.Tensor = torch.tensor(0.7),\n",
    "# ):\n",
    "#     x = params[\"x\"]\n",
    "#     diff1 = x - means1\n",
    "#     log_prob1 = 0.5 * torch.dot(diff1, torch.linalg.solve(cov, diff1))\n",
    "#\n",
    "#     diff2 = x - means2\n",
    "#     log_prob2 = 0.5 * torch.dot(diff2, torch.linalg.solve(cov, diff2))\n",
    "#     return -torch.log(const * torch.exp(-log_prob1) + (1-const) * torch.exp(-log_prob2))\n",
    "#\n",
    "# prob({\"x\": torch.zeros((ndim,), device=DEV)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nwalkers = 2\n",
    "# p0 = torch.randn((nwalkers, ndim), device=DEV)\n",
    "# p0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prob({\"x\": p0[0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kernel = NUTS(\n",
    "#     potential_fn=prob,\n",
    "#     jit_compile=True,\n",
    "#     target_accept_prob=0.6,\n",
    "#     # full_mass=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mcmc = MCMC(kernel, warmup_steps=1000, num_samples=10_000, initial_params={\"x\": p0}, num_chains=nwalkers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mcmc.run(means1, means2, cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mcmc.diagnostics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mcmc.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples = mcmc.get_samples()[\"x\"]\n",
    "# samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "#\n",
    "# samples_ = samples.cpu()\n",
    "# plt.hist(samples_[:, 0], 100, color=\"k\", histtype=\"step\")\n",
    "# plt.xlabel(r\"$\\theta_1$\")\n",
    "# plt.ylabel(r\"$p(\\theta_1)$\")\n",
    "# plt.gca().set_yticks([]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# x = samples[:, 0].cpu()\n",
    "# y = samples[:, 1].cpu()\n",
    "# g = sns.jointplot(x=x, y=y, alpha=0.1)\n",
    "# g.plot_joint(sns.kdeplot, color=\"r\")\n",
    "# g.plot_marginals(sns.rugplot, color=\"r\", height=-.15, clip_on=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @torch.jit.script\n",
    "def log_prior(\n",
    "    z,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Corresponds to the log prior of a Normal(z; 0, 1) distribution.\n",
    "    :param params:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    z = z.squeeze()\n",
    "    z_2 = z**2\n",
    "    return 0.5 * torch.sum(z_2)\n",
    "\n",
    "\n",
    "# log_prior = torch.jit.script(_log_prior, example_inputs=[({\"z\": noise_sampler(1)},)])\n",
    "\n",
    "log_prior(noise_sampler(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood_latent(\n",
    "    z,\n",
    "    data=data,\n",
    "    generator=G,\n",
    "    transform_out=transform_out_,\n",
    "):\n",
    "    eps = torch.finfo(z.dtype).eps\n",
    "\n",
    "    z = torch.reshape(z, (1, -1, 1, 1))\n",
    "    with torch.no_grad():\n",
    "        m = generator(z)\n",
    "    m = transform_out(m)\n",
    "    m = m.reshape((-1,))\n",
    "\n",
    "    m_data = m.take(data)\n",
    "    # m_data_zeros = m_data == 0\n",
    "    m_data = m_data + eps  # to avoid log(0)\n",
    "    logits = torch.log(m_data)  # log m(x_i)\n",
    "    # logits[m_data_zeros] = logits[m_data_zeros] * 3\n",
    "\n",
    "    return torch.sum(logits)  # \\sum_{i=1}^n \\log m(x_i)\n",
    "\n",
    "\n",
    "# log_likelihood_latent = torch.jit.script(_log_likelihood_latent, example_inputs=[({\"z\": noise_sampler(1)}, data, G, transform_out_)])\n",
    "\n",
    "log_likelihood_latent(noise_sampler(1), data, G, transform_out_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_posterior(\n",
    "    z,\n",
    "    data=data,\n",
    "    generator=G,\n",
    "    transform_out=transform_out_,\n",
    "):\n",
    "    return log_prior(z) + log_likelihood_latent(\n",
    "        z, data, generator, transform_out\n",
    "    )\n",
    "\n",
    "\n",
    "log_posterior(noise_sampler(1), data, G, transform_out_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nwalkers = 2\n",
    "# z0 = noise_sampler(nwalkers).squeeze()\n",
    "# z0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kernel = NUTS(\n",
    "#     potential_fn=log_posterior,\n",
    "#     # jit_compile=True,\n",
    "#     target_accept_prob=0.6,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mcmc = MCMC(\n",
    "#     kernel,\n",
    "#     warmup_steps=1_000,\n",
    "#     num_samples=10_000,\n",
    "#     initial_params={\"z\": z0},\n",
    "#     num_chains=nwalkers,\n",
    "#     mp_context=\"spawn\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mcmc.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z = noise_sampler(1).squeeze()\n",
    "# z_abs = torch.abs(z)\n",
    "# constraint = torch.maximum(z_abs - 1, torch.zeros_like(z_abs))\n",
    "# penalizations = 1e6 * constraint ** 2\n",
    "# constraint, penalizations.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import typing as t\n",
    "#\n",
    "#\n",
    "# @torch.jit.script\n",
    "# def _log_prior_unif(z: torch.Tensor, radius=torch.tensor(1), penalizaton=torch.tensor(1e6)):\n",
    "#     \"\"\"\n",
    "#     Compute the log-prior of the latent variable z. The prior is uniform [-1, 1].\n",
    "#     \"\"\"\n",
    "#     z = z.squeeze()\n",
    "#     z_abs = torch.abs(z)\n",
    "#     constraint = torch.maximum(z_abs - radius, torch.zeros_like(z_abs))\n",
    "#     penalizations = penalizaton * constraint ** 2\n",
    "#     return -torch.sum(penalizations)\n",
    "#\n",
    "#\n",
    "# @torch.jit.script\n",
    "# def _log_prior_norm(z: torch.Tensor, radius=torch.tensor(3), penalizaton=torch.tensor(1e6)):\n",
    "#     \"\"\"\n",
    "#     Compute the log-prior of the latent variable z.\n",
    "#     \"\"\"\n",
    "#     z = z.squeeze()\n",
    "#     # min_value = torch.finfo(z.dtype).min\n",
    "#     norm_z_2 = torch.sum(z ** 2)  # \\|z\\|^2\n",
    "#     n = z.shape[0]\n",
    "#     constraint = torch.maximum(norm_z_2 / n - radius ** 2, torch.zeros_like(norm_z_2))\n",
    "#     penalizations = penalizaton * constraint ** 2\n",
    "#     return -norm_z_2 / 2 - penalizations  # -\\frac{1}{2} \\|z\\|^2\n",
    "#\n",
    "#\n",
    "# # @torch.jit.script\n",
    "# def _log_prior(z: torch.Tensor, G, radius=torch.tensor(1), penalizaton=torch.tensor(1e6)):\n",
    "#     \"\"\"\n",
    "#     Compute the log-prior of the latent variable z.\n",
    "#     \"\"\"\n",
    "#     if G._latent_distr(1).name == \"unif\":\n",
    "#         return _log_prior_unif(z, radius, penalizaton)\n",
    "#     elif G._latent_distr(1).name == \"norm\":\n",
    "#         return _log_prior_norm(z, radius, penalizaton)\n",
    "#     raise ValueError(f\"unknown latent distribution {G._latent_distr(1).name}\")\n",
    "#\n",
    "#\n",
    "# # @torch.jit.script\n",
    "# def _log_likelihood_latent(z: torch.Tensor, data: torch.Tensor, generator: torch.jit.ScriptModule, transform_out: t.Callable[[torch.Tensor], torch.Tensor]):\n",
    "#     \"\"\"\n",
    "#     Compute the log-likelihood of the data given the latent variable z.\n",
    "#     This is done by first generating the image x from z, then transforming it to a DistributionDraw object.\n",
    "#\n",
    "#     The original likelihood is:\n",
    "#     .. math::\n",
    "#         \\Pi_n(dm)\n",
    "#         \\propto \\Pi(dm) \\mathcal{L}_n(m)\n",
    "#         = \\int_{\\mathcal{Z}} P_Z(dz) \\Pi(dm | z) \\mathcal{L}_n(m)\n",
    "#         \\propto \\int_{\\mathcal{Z}} dz \\Pi(dm | z) \\mathcal{L}_n(m) e^{-\\frac{1}{2} \\|z\\|^2}\n",
    "#         = \\int_{\\mathcal{Z}} dz \\Pi(dm | z) \\prod_{i=1}^n m(x_i) e^{-\\frac{1}{2} \\|z\\|^2}\n",
    "#\n",
    "#     So, the log-likelihood with respecto to z is:\n",
    "#     .. math::\n",
    "#         \\ell_n(z) = \\sum_{i=1}^n \\log m(x_i) - \\frac{1}{2} \\|z\\|^2\n",
    "#     \"\"\"\n",
    "#     eps = torch.finfo(z.dtype).eps\n",
    "#\n",
    "#     z = torch.reshape(z, (1, -1, 1, 1))\n",
    "#     with torch.no_grad():\n",
    "#         m = generator(z)\n",
    "#     m = transform_out(m)\n",
    "#     m = m.reshape((-1,))\n",
    "#\n",
    "#     # m_data = m.take(data) + eps  # to avoid log(0)\n",
    "#     # logits = torch.log(m_data)  # log m(x_i)\n",
    "#\n",
    "#     m_data = m.take(data)\n",
    "#     # m_data_zeros = m_data == 0\n",
    "#     m_data = m_data + eps  # to avoid log(0)\n",
    "#     logits = torch.log(m_data)  # log m(x_i)\n",
    "#     # logits[m_data_zeros] = logits[m_data_zeros] * 3\n",
    "#\n",
    "#     return torch.sum(logits)  # \\sum_{i=1}^n \\log m(x_i)\n",
    "#\n",
    "#\n",
    "# # @torch.jit.script\n",
    "# def _log_posterior(\n",
    "#     z: torch.Tensor,\n",
    "#     data: torch.Tensor,\n",
    "#     generator: torch.nn.Module,\n",
    "#     transform_out,\n",
    "#     radius=torch.tensor(1),\n",
    "#     penalization=torch.tensor(1e6)\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Compute the log-posterior of the latent variable z.\n",
    "#     .. math::\n",
    "#         \\log \\Pi_n(z) = \\log \\Pi_Z(z) + \\log \\Pi_n(dm)\n",
    "#     \"\"\"\n",
    "#     return _log_prior_unif(z, radius, penalization) + _log_likelihood_latent(z, data, generator, transform_out)\n",
    "#\n",
    "#\n",
    "# z = noise_sampler(1).squeeze()\n",
    "#\n",
    "# _log_likelihood_latent(z, data, G, transform_out), _log_posterior(z, data, G, transform_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @torch.jit.script\n",
    "# def _log_likelihood_true_latent(z: torch.Tensor, data: torch.Tensor, generator: torch.nn.Module, transform_out):\n",
    "#     \"\"\"\n",
    "#     Compute the log-likelihood of the data given the latent variable z.\n",
    "#     This is done by first generating the image x from z, then transforming it to a DistributionDraw object.\n",
    "#\n",
    "#     The original likelihood is:\n",
    "#     .. math::\n",
    "#         \\Pi_n(dm)\n",
    "#         \\propto \\Pi(dm) \\mathcal{L}_n(m)\n",
    "#         = \\int_{\\mathcal{Z}} P_Z(dz) \\Pi(dm | z) \\mathcal{L}_n(m)\n",
    "#         \\propto \\int_{\\mathcal{Z}} dz \\Pi(dm | z) \\mathcal{L}_n(m) e^{-\\frac{1}{2} \\|z\\|^2}\n",
    "#         = \\int_{\\mathcal{Z}} dz \\Pi(dm | z) \\prod_{i=1}^n m(x_i) e^{-\\frac{1}{2} \\|z\\|^2}\n",
    "#\n",
    "#     So, the log-likelihood with respecto to z is:\n",
    "#     .. math::\n",
    "#         \\ell_n(z) = \\sum_{i=1}^n \\log m(x_i) - \\frac{1}{2} \\|z\\|^2\n",
    "#     \"\"\"\n",
    "#     eps = torch.finfo(z.dtype).eps\n",
    "#\n",
    "#     z = torch.reshape(z, (1, -1, 1, 1))\n",
    "#     with torch.no_grad():\n",
    "#         m = generator(z)\n",
    "#     m = transform_out(m)\n",
    "#     m = m.reshape((-1,))\n",
    "#\n",
    "#     m_data = m.take(data)\n",
    "#     m_data_zeros = m_data == 0\n",
    "#     m_data = m_data + eps  # to avoid log(0)\n",
    "#     logits = torch.log(m_data)  # log m(x_i)\n",
    "#     logits[m_data_zeros] = logits[m_data_zeros] * 10\n",
    "#\n",
    "#     return torch.sum(logits)  # \\sum_{i=1}^n \\log m(x_i)\n",
    "#\n",
    "#\n",
    "# _log_likelihood_true_latent(z, data, G, transform_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for _ in range(10):\n",
    "#     z = G.sample_noise(1).squeeze()\n",
    "#     print(_log_likelihood_true_latent(z, data, G, transform_out), _log_prior(z), _log_posterior(z, data, G, transform_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCMC Clase Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bwb.distributions import BaseGeneratorDistribSampler\n",
    "#\n",
    "#\n",
    "# class BaseLatentMCMCPosteriorSampler(BaseGeneratorDistribSampler[DistributionDraw]):\n",
    "#     def __init__(self):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCMC Clase Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from hamiltorch import Sampler, Integrator, Metric\n",
    "\n",
    "from bwb.distributions.posterior_samplers import (\n",
    "    BaseLatentMCMCPosteriorSampler as _LatentMCMCPosteriorPiN,\n",
    ")\n",
    "\n",
    "\n",
    "_LatentMCMCPosteriorPiN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCMC paralelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bwb.distributions.posterior_samplers import (\n",
    "    LatentMCMCPosteriorSampler as LatentMCMCPosteriorPiN,\n",
    ")\n",
    "\n",
    "\n",
    "LatentMCMCPosteriorPiN(parallel=True), LatentMCMCPosteriorPiN(\n",
    "    n_workers=2\n",
    "), LatentMCMCPosteriorPiN(n_walkers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nu-U-Turn-Sampler (NUTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "burn = 5_000\n",
    "num_samples = 500_000\n",
    "n_walkers = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bwb.utils as bwb_utils\n",
    "from bwb.distributions.posterior_samplers import (\n",
    "    NUTSPosteriorSampler as NUTSPosteriorPiN,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "if not NUTS_POSTERIOR_PATH.exists() or RUN_MCMC:\n",
    "    post_pi_n = (\n",
    "        NUTSPosteriorPiN(\n",
    "            n_walkers=n_walkers,\n",
    "            num_steps_per_sample=1,\n",
    "            burn=burn,\n",
    "            desired_accept_rate=0.6,\n",
    "        )\n",
    "        .fit(G, transform_out_, noise_sampler, data[:100])\n",
    "        .run(n_steps=num_samples)\n",
    "    )\n",
    "    post_pi_n.save(NUTS_POSTERIOR_PATH)\n",
    "else:\n",
    "    post_pi_n = NUTSPosteriorPiN.load(NUTS_POSTERIOR_PATH)\n",
    "\n",
    "post_pi_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_autocorr_time = int(post_pi_n.get_autocorr_time().mean())\n",
    "print(mean_autocorr_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_pi_n.shuffle_samples_cache(thin=int(mean_autocorr_time / 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_images = 12\n",
    "plotters.plot_list_of_draws(post_pi_n_.rvs(max_images), n_rows=2, n_cols=6)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_pi_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wgan_gp.wgan_gp_vae.utils import ProjectorOnManifold\n",
    "import torchvision.transforms as T\n",
    "\n",
    "transform_in_proj = T.Compose([\n",
    "    # From pdf to grayscale\n",
    "    T.Lambda(lambda x: x / torch.max(x)),\n",
    "    T.ToPILImage(),\n",
    "    T.Resize((32, 32)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(\n",
    "        [0.5 for _ in range(1)],\n",
    "        [0.5 for _ in range(1)],\n",
    "    ),\n",
    "])\n",
    "\n",
    "transform_out_proj = T.Compose([\n",
    "    # Ensure the range is in [0, 1]\n",
    "    T.Lambda(lambda x: x - torch.min(x)),\n",
    "    T.Lambda(lambda x: x / torch.max(x)),\n",
    "    T.Lambda(lambda x: x / torch.sum(x)),\n",
    "    T.Lambda(lambda x: x.squeeze(0)),\n",
    "])\n",
    "\n",
    "proj = ProjectorOnManifold(\n",
    "    E,\n",
    "    G,\n",
    "    transform_in=transform_in_proj,\n",
    "    transform_out=transform_out_proj,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bwb.sgdw.sgdw as sgdw\n",
    "from bwb.sgdw.utils import gamma\n",
    "from bwb.sgdw.plotters import PlotterComparison\n",
    "\n",
    "dist_draw_sgdw = sgdw.DebiesedDistributionDrawSGDW(\n",
    "    distr_sampler=post_pi_n,\n",
    "    step_scheduler=gamma(a=0.5, b=0.51, c=0.51),\n",
    "    projector=proj,\n",
    "    proj_every=5,\n",
    "    max_iter=len(post_pi_n.samples_cache) - 5,\n",
    "    report_every=10,\n",
    ").set_geodesic_params(\n",
    "    reg=0.01,\n",
    "    stop_thr=1e-3,\n",
    ")\n",
    "\n",
    "plotter_comp = PlotterComparison(\n",
    "    dist_draw_sgdw, plot_every=50, n_cols=12, n_rows=2, cmap=\"binary_r\"\n",
    ")\n",
    "\n",
    "bar = plotter_comp.run(\n",
    "    include_dict=dict(total_time=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "plotter_comp.plot(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_images = 36\n",
    "# plotters.plot_list_of_images(post_pi_n.rvs(max_images))\n",
    "# bwb_utils.plot_list_of_draws(post_pi_n.rvs(max_images), max_images=max_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_pi_n_dict = {}\n",
    "times_autocorr = {}\n",
    "total_times = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "burn = 400\n",
    "num_samples = 15_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar = \"=\" * 10\n",
    "\n",
    "for step_p_sample in [1, 2, 3]:\n",
    "    post_pi_n_dict[step_p_sample] = {}\n",
    "    times_autocorr[step_p_sample] = {}\n",
    "    total_times[step_p_sample] = {}\n",
    "\n",
    "    for n_data in [5, 50, 100]:\n",
    "        post_pi_n_dict[step_p_sample][n_data] = {}\n",
    "        times_autocorr[step_p_sample][n_data] = {}\n",
    "        total_times[step_p_sample][n_data] = {}\n",
    "\n",
    "        for desired_acc_rate in [0.3, 0.6]:\n",
    "            print(\n",
    "                bar\n",
    "                + f\" {step_p_sample = }, {n_data = }, {desired_acc_rate = } \"\n",
    "                + bar\n",
    "            )\n",
    "            post_pi_n = NUTSPosteriorPiN(\n",
    "                n_walkers=8,\n",
    "                num_steps_per_sample=step_p_sample,\n",
    "                burn=burn,\n",
    "                desired_accept_rate=desired_acc_rate,\n",
    "            )\n",
    "            post_pi_n.fit(data[:n_data], G, transform_out)\n",
    "            post_pi_n.run(num_samples=num_samples)\n",
    "\n",
    "            mean_autocorr_time = int(post_pi_n.get_autocorr_time().mean())\n",
    "            total_time = post_pi_n.total_time\n",
    "\n",
    "            post_pi_n_dict[step_p_sample][n_data][desired_acc_rate] = post_pi_n\n",
    "            times_autocorr[step_p_sample][n_data][\n",
    "                desired_acc_rate\n",
    "            ] = mean_autocorr_time\n",
    "            total_times[step_p_sample][n_data][desired_acc_rate] = total_time\n",
    "\n",
    "            print(post_pi_n)\n",
    "            print(f\"{mean_autocorr_time = }\")\n",
    "            print(f\"{total_time = }\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se demorÃ³ 13.85 horas en correr la celda anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_data in [5, 50, 100]:\n",
    "    for desired_acc_rate in [0.6, 0.3]:\n",
    "        for step_p_sample in [1, 2, 3]:\n",
    "            print(\n",
    "                bar\n",
    "                + f\" {step_p_sample = }, {n_data = }, {desired_acc_rate = } \"\n",
    "                + bar\n",
    "            )\n",
    "            # print(post_pi_n_dict[step_p_sample][n_data][desired_acc_rate])\n",
    "            print(\n",
    "                \"autocorr time:\"\n",
    "                f\" {times_autocorr[step_p_sample][n_data][desired_acc_rate]}\"\n",
    "            )\n",
    "            print(\n",
    "                \"time spend:\"\n",
    "                f\" {total_times[step_p_sample][n_data][desired_acc_rate] / 60:.2f} mins\"\n",
    "            )\n",
    "            print()\n",
    "        print()\n",
    "    print(bar * 8 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AquÃ­ tomamos aquellas cadenas que tuvieron un tiempo de autocorrelaciÃ³n menor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_pi_n_5 = post_pi_n_dict[1][5][0.3]\n",
    "post_pi_n_50 = post_pi_n_dict[2][50][0.3]\n",
    "post_pi_n_100 = post_pi_n_dict[3][100][0.3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenemos sus tiempos de autocorrelaciÃ³n promedios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_autocorr_time_n_5 = int(post_pi_n_5.get_autocorr_time().mean())\n",
    "print(f\"{mean_autocorr_time_n_5 = }\")\n",
    "\n",
    "mean_autocorr_time_n_50 = int(post_pi_n_50.get_autocorr_time().mean())\n",
    "print(f\"{mean_autocorr_time_n_50 = }\")\n",
    "\n",
    "mean_autocorr_time_n_100 = int(post_pi_n_100.get_autocorr_time().mean())\n",
    "print(f\"{mean_autocorr_time_n_100 = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora haremos muestreos utilizando el tiempo de autocorrelaciÃ³n por cada cadena."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_pi_n_5.shuffle_samples_cache(thin=mean_autocorr_time_n_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_pi_n_50.shuffle_samples_cache(thin=mean_autocorr_time_n_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_pi_n_100.shuffle_samples_cache(thin=mean_autocorr_time_n_100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y visualizaremos los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_images = 4 * 16\n",
    "bwb_utils.plot_list_of_draws(post_pi_n_5.rvs(max_images), max_images=max_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bwb_utils.plot_list_of_draws(\n",
    "    post_pi_n_50.rvs(max_images), max_images=max_images\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bwb_utils.plot_list_of_draws(\n",
    "    post_pi_n_100.rvs(max_images), max_images=max_images\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hamiltonian Monte Carlo (HMC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HMCPosteriorPiN(LatentMCMCPosteriorPiN):\n",
    "    def __init__(\n",
    "        self,\n",
    "        log_prob_fn=_log_posterior,\n",
    "        num_samples=10,\n",
    "        num_steps_per_sample=5,\n",
    "        burn=10,\n",
    "        step_size=0.1,\n",
    "        **kwargs,\n",
    "    ) -> None:\n",
    "        super().__init__(\n",
    "            log_prob_fn=log_prob_fn,\n",
    "            num_samples=num_samples,\n",
    "            num_steps_per_sample=num_steps_per_sample,\n",
    "            burn=burn,\n",
    "            step_size=step_size,\n",
    "            sampler=Sampler.HMC,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "\n",
    "post_pi_n = HMCPosteriorPiN()\n",
    "post_pi_n.fit(data[:5], G, transform_out)\n",
    "\n",
    "max_images = 4 * 9\n",
    "post_pi_n.reset_samples().run(num_samples=max_images)\n",
    "\n",
    "bwb_utils.plot_list_of_draws(post_pi_n.rvs(max_images), max_images=max_images)\n",
    "\n",
    "print(post_pi_n)\n",
    "print(post_pi_n.total_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Riemannian Manifold Hamiltonian Monte Carlo (RMHMC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMHMCPosteriorPiN(LatentMCMCPosteriorPiN):\n",
    "    def __init__(\n",
    "        self,\n",
    "        log_prob_fn=_log_posterior,\n",
    "        num_samples=10,\n",
    "        num_steps_per_sample=5,\n",
    "        burn=10,\n",
    "        step_size=0.1,\n",
    "        fixed_point_max_iterations=1000,\n",
    "        fixed_point_threshold=1e-5,\n",
    "        explicit_binding_const=100,\n",
    "        integrator=Integrator.IMPLICIT,\n",
    "        metric=Metric.HESSIAN,\n",
    "        **kwargs,\n",
    "    ) -> None:\n",
    "        super().__init__(\n",
    "            log_prob_fn=log_prob_fn,\n",
    "            num_samples=num_samples,\n",
    "            num_steps_per_sample=num_steps_per_sample,\n",
    "            burn=burn,\n",
    "            step_size=step_size,\n",
    "            fixed_point_max_iterations=fixed_point_max_iterations,\n",
    "            fixed_point_threshold=fixed_point_threshold,\n",
    "            explicit_binding_const=explicit_binding_const,\n",
    "            sampler=Sampler.RMHMC,\n",
    "            integrator=integrator,\n",
    "            metric=metric,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    def _additional_repr_(self, sep):\n",
    "        to_return = super()._additional_repr_(sep)\n",
    "        to_return += f\"metric={self.hamiltorch_kwargs['metric'].name}\" + sep\n",
    "        return to_return\n",
    "\n",
    "\n",
    "post_pi_n = RMHMCPosteriorPiN(step_size=0.1)\n",
    "post_pi_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implicit RMHMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImplicitRMHMCPosteriorPiN(RMHMCPosteriorPiN):\n",
    "    def __init__(\n",
    "        self,\n",
    "        log_prob_fn=_log_posterior,\n",
    "        num_samples=10,\n",
    "        num_steps_per_sample=10,\n",
    "        burn=10,\n",
    "        step_size=0.1,\n",
    "        fixed_point_max_iterations=1000,\n",
    "        fixed_point_threshold=1e-5,\n",
    "        metric=Metric.HESSIAN,\n",
    "        **kwargs,\n",
    "    ) -> None:\n",
    "        super().__init__(\n",
    "            log_prob_fn=log_prob_fn,\n",
    "            num_samples=num_samples,\n",
    "            num_steps_per_sample=num_steps_per_sample,\n",
    "            burn=burn,\n",
    "            step_size=step_size,\n",
    "            integrator=Integrator.IMPLICIT,\n",
    "            metric=metric,\n",
    "            fixed_point_max_iterations=fixed_point_max_iterations,\n",
    "            fixed_point_threshold=fixed_point_threshold,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    def _additional_repr_(self, sep):\n",
    "        to_return = super()._additional_repr_(sep)\n",
    "        to_return += (\n",
    "            f\"fixed_point_max_iterations={self.hamiltorch_kwargs['fixed_point_max_iterations']}\"\n",
    "            + sep\n",
    "        )\n",
    "        to_return += (\n",
    "            f\"fixed_point_threshold={self.hamiltorch_kwargs['fixed_point_threshold']}\"\n",
    "            + sep\n",
    "        )\n",
    "        return to_return\n",
    "\n",
    "\n",
    "post_pi_n = ImplicitRMHMCPosteriorPiN(num_steps_per_sample=1)\n",
    "post_pi_n.fit(data[:5], G, transform_out)\n",
    "\n",
    "max_images = 4 * 9\n",
    "num_samples = max_images\n",
    "post_pi_n.reset_samples().run(num_samples=num_samples)\n",
    "\n",
    "bwb_utils.plot_list_of_draws(post_pi_n.rvs(max_images), max_images=max_images)\n",
    "\n",
    "print(post_pi_n)\n",
    "print(post_pi_n.total_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explicit RMHMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExplicitRMHMCPosteriorPiN(RMHMCPosteriorPiN):\n",
    "    def __init__(\n",
    "        self,\n",
    "        log_prob_fn=_log_posterior,\n",
    "        num_samples=10,\n",
    "        num_steps_per_sample=10,\n",
    "        burn=10,\n",
    "        step_size=0.1,\n",
    "        explicit_binding_const=100,\n",
    "        metric=Metric.HESSIAN,\n",
    "        **kwargs,\n",
    "    ) -> None:\n",
    "        super().__init__(\n",
    "            log_prob_fn=log_prob_fn,\n",
    "            num_samples=num_samples,\n",
    "            num_steps_per_sample=num_steps_per_sample,\n",
    "            burn=burn,\n",
    "            step_size=step_size,\n",
    "            integrator=Integrator.EXPLICIT,\n",
    "            metric=metric,\n",
    "            explicit_binding_const=explicit_binding_const,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    def _additional_repr_(self, sep):\n",
    "        to_return = super()._additional_repr_(sep)\n",
    "        to_return += (\n",
    "            f\"explicit_binding_const={self.hamiltorch_kwargs['explicit_binding_const']}\"\n",
    "            + sep\n",
    "        )\n",
    "        return to_return\n",
    "\n",
    "\n",
    "post_pi_n = ExplicitRMHMCPosteriorPiN()\n",
    "post_pi_n.fit(data[:5], G, transform_out)\n",
    "\n",
    "max_images = 4 * 9\n",
    "post_pi_n.reset_samples().run(num_samples=max_images)\n",
    "\n",
    "bwb_utils.plot_list_of_draws(post_pi_n.rvs(max_images), max_images=max_images)\n",
    "\n",
    "print(post_pi_n)\n",
    "print(post_pi_n.total_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "\n",
    "\n",
    "def mcmc_experiment(\n",
    "    data,\n",
    "    num_steps_per_sample,\n",
    "    burn=50,\n",
    "    G=G,\n",
    "    transform_out=transform_out,\n",
    "    max_images=4 * 9,\n",
    "):\n",
    "    try:\n",
    "        post_pi_n = NUTSPosteriorPiN(\n",
    "            burn=burn,\n",
    "            num_steps_per_sample=num_steps_per_sample,\n",
    "            desired_accept_rate=0.3,\n",
    "        )\n",
    "        post_pi_n.fit(data, G, transform_out)\n",
    "\n",
    "        post_pi_n.reset_samples().run(num_samples=max_images)\n",
    "\n",
    "        bwb_utils.plot_list_of_draws(\n",
    "            post_pi_n.rvs(max_images), max_images=max_images\n",
    "        )\n",
    "\n",
    "        print(f\"{post_pi_n = }\")\n",
    "        print(f\"{post_pi_n.total_time = :.4f}\")\n",
    "    except:\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Num steps per sample = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps_ps = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N Data = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data = 5\n",
    "mcmc_experiment(data[:n_data], num_steps_per_sample=num_steps_ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N Data = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data = 10\n",
    "mcmc_experiment(data[:n_data], num_steps_per_sample=num_steps_ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N Data = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data = 25\n",
    "mcmc_experiment(data[:n_data], num_steps_per_sample=num_steps_ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N Data = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data = 50\n",
    "mcmc_experiment(data[:n_data], num_steps_per_sample=num_steps_ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Num steps per sample = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps_ps = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N Data = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data = 5\n",
    "mcmc_experiment(data[:n_data], num_steps_per_sample=num_steps_ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N Data = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data = 10\n",
    "mcmc_experiment(data[:n_data], num_steps_per_sample=num_steps_ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N Data = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data = 25\n",
    "mcmc_experiment(data[:n_data], num_steps_per_sample=num_steps_ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N Data = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data = 50\n",
    "mcmc_experiment(data[:n_data], num_steps_per_sample=num_steps_ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Num steps per sample = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps_ps = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N Data = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data = 5\n",
    "mcmc_experiment(data[:n_data], num_steps_per_sample=num_steps_ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N Data = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data = 10\n",
    "mcmc_experiment(data[:n_data], num_steps_per_sample=num_steps_ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N Data = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data = 25\n",
    "mcmc_experiment(data[:n_data], num_steps_per_sample=num_steps_ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N Data = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data = 50\n",
    "mcmc_experiment(data[:n_data], num_steps_per_sample=num_steps_ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Num steps per sample = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps_ps = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N Data = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data = 5\n",
    "mcmc_experiment(data[:n_data], num_steps_per_sample=num_steps_ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N Data = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data = 10\n",
    "mcmc_experiment(data[:n_data], num_steps_per_sample=num_steps_ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N Data = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data = 25\n",
    "mcmc_experiment(data[:n_data], num_steps_per_sample=num_steps_ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N Data = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data = 50\n",
    "mcmc_experiment(data[:n_data], num_steps_per_sample=num_steps_ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Num steps per sample = 1.000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps_ps = 1_000\n",
    "burn = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N Data = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data = 5\n",
    "mcmc_experiment(data[:n_data], num_steps_per_sample=num_steps_ps, burn=burn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N Data = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data = 10\n",
    "mcmc_experiment(data[:n_data], num_steps_per_sample=num_steps_ps, burn=burn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N Data = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data = 25\n",
    "mcmc_experiment(data[:n_data], num_steps_per_sample=num_steps_ps, burn=burn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N Data = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data = 50\n",
    "mcmc_experiment(data[:n_data], num_steps_per_sample=num_steps_ps, burn=burn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Num steps per sample = 10.000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps_ps = 10_000\n",
    "burn = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N Data = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data = 5\n",
    "mcmc_experiment(data[:n_data], num_steps_per_sample=num_steps_ps, burn=burn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N Data = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data = 10\n",
    "mcmc_experiment(data[:n_data], num_steps_per_sample=num_steps_ps, burn=burn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N Data = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data = 25\n",
    "mcmc_experiment(data[:n_data], num_steps_per_sample=num_steps_ps, burn=burn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N Data = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_data = 50\n",
    "mcmc_experiment(data[:n_data], num_steps_per_sample=num_steps_ps, burn=burn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
